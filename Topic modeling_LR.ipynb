{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import nltk\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "from collections import defaultdict\n",
      "import operator\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.set_option('display.height', 300)\n",
      "pd.set_option('display.max_rows', 500)\n",
      "pd.set_option('display.max_columns', 50)\n",
      "pd.set_option('display.width', 200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "import nltk.metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import nltk\n",
      "import numpy as np\n",
      "import gensim\n",
      "import random\n",
      "from datetime import datetime\n",
      "from gensim import corpora, models, similarities\n",
      "from nltk.tokenize.punkt import PunktWordTokenizer\n",
      "\n",
      "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "import unicodedata\n",
      "\n",
      "def unaccent( text ):\n",
      "    s = unicodedata.normalize( 'NFKD', text )\n",
      "    s = ''.join( c for c in s if ord( c ) < 127 )\n",
      "    return s\n",
      "\n",
      "stemmer = PorterStemmer()\n",
      "lmtzr = WordNetLemmatizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn\n",
      "import itertools\n",
      "nltk.tag._POS_TAGGER\n",
      "\n",
      "def get_wordnet_pos(treebank_tag):\n",
      "    if treebank_tag.startswith('J'):\n",
      "        return wn.ADJ\n",
      "    elif treebank_tag.startswith('V'):\n",
      "        return wn.VERB\n",
      "    elif treebank_tag.startswith('N'):\n",
      "        return wn.NOUN\n",
      "    elif treebank_tag.startswith('R'):\n",
      "        return wn.ADV\n",
      "    else:\n",
      "        return wn.NOUN\n",
      "\n",
      "\n",
      "def get_first_synonym(lemma0, pos):\n",
      "    synonyms = []\n",
      "    if len(wn.synsets(lemma0, pos)) > 0:\n",
      "        for synset in wn.synsets(lemma0, pos):\n",
      "            synonyms.append(synset.lemma_names)\n",
      "            merged = list(itertools.chain(*synonyms))\n",
      "            merged.sort()\n",
      "        return merged[0].lower()\n",
      "    else:\n",
      "        return lemma0.lower()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_rw = pd.read_csv(\"C:\\Users\\shubham.akshat\\Desktop\\BODM/remove_words.txt\", sep = \"\\t\")\n",
      "remove_words = df_rw[\"remove_words\"].tolist()\n",
      "remove_words_lst = []\n",
      "for item in remove_words:\n",
      "    remove_words_lst.append(item.lower())\n",
      "\n",
      "df_wm = pd.read_csv(\"C:\\Users\\shubham.akshat\\Desktop\\BODM/word_mapping.txt\", sep = \"\\t\")\n",
      "incorrect_words = df_wm[\"incorrect\"].tolist()\n",
      "correct_words = df_wm[\"correct\"].tolist()\n",
      "\n",
      "def get_correct_word(word):\n",
      "    index = incorrect_words.index(word)\n",
      "    return correct_words[index]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stem_tokens(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        stemmed.append(stemmer.stem(item))\n",
      "    return stemmed\n",
      "\n",
      "def lemma_tokens(tokens, lmtzr):\n",
      "    lemma_lst = []\n",
      "    for word in tokens:\n",
      "        lexicon = nltk.pos_tag([word])\n",
      "        POS = get_wordnet_pos(lexicon[0][1])        \n",
      "        lemma_lst.append(lmtzr.lemmatize(word, POS))\n",
      "    return lemma_lst\n",
      "\n",
      "\n",
      "def tokenize(text):\n",
      "    \n",
      "    tokens = nltk.word_tokenize(text)\n",
      "   # filtered_tokens = [w for w in tokens if not w in stopwords.words('english')]\n",
      "\n",
      "    \n",
      "    lexicons = nltk.pos_tag(tokens)\n",
      "    lemmatised_tokens = []\n",
      "    for lexicon in lexicons:\n",
      "        if lexicon[0].lower() in incorrect_words:\n",
      "            lexicon = (get_correct_word(lexicon[0]), lexicon[1])\n",
      "        if lexicon[0].lower() not in remove_words and lexicon[0] not in stopwords.words('english'):\n",
      "            wn_pos = get_wordnet_pos(lexicon[1])\n",
      "            lemma0 = lmtzr.lemmatize(lexicon[0].lower(), wn_pos)\n",
      "            lemma1 = get_first_synonym(lemma0, wn_pos)\n",
      "            lemmatised_tokens.append(lemma1)\n",
      "    \n",
      " #   lemmatised_tokens = [w for w in lemmatised_tokens_1 if not w in remove_words]\n",
      "    \n",
      "#    return stemmed_tokens\n",
      "    return lemmatised_tokens\n",
      "#    return filtered_tokens\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(\"C:\\Users\\shubham.akshat\\Desktop\\BODM/TELEGRAPH__RMA__BODM.txt\", sep = \"\\t\")\n",
      "\n",
      "df[\"free_text\"] = df.PROBLEM_DESCRIPTION.str.lower()\n",
      "df = df[df[\"HDX_CODEC\"] == True]\n",
      "\n",
      "df = df[pd.notnull(df['free_text'])]\n",
      "df = df.drop_duplicates(cols = 'free_text')\n",
      "df.index = range(len(df))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "replace_punctuation = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
      "print \"Tokenizing Started at    :    \", datetime.now()\n",
      "\n",
      "texts = []\n",
      "for i in range(len(df)):\n",
      "    temp1 = str(df[\"free_text\"][i]).translate(replace_punctuation)\n",
      "    temp2 = temp1.decode('utf-8','ignore')\n",
      "    temp3 = tokenize(temp2)\n",
      "    temp4 = []\n",
      "    flag = {}\n",
      "    for word in temp3:\n",
      "        if str(word) in [\"ec04\", \"ec05\", \"ec06\", \"ec07\", \"ec08\", \"ec09\"]:\n",
      "            flag[word] = 1\n",
      "        elif any(char.isdigit() for char in word) or len(word) < 3:\n",
      "            flag[word] = 0\n",
      "        else:\n",
      "            flag[word] = 1\n",
      "    for word, flg in flag.items():\n",
      "        if flg == 1:\n",
      "            temp4.append(word)\n",
      "        \n",
      "    texts.append(temp4)\n",
      "\n",
      "print \"Tokenizing ended at    :    \", datetime.now()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-11-e1f78051440b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtemp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"free_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace_punctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtemp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtemp3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtemp4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-7-638399c7264e>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mlexicons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mlemmatised_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlexicons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\tag\\__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \"\"\"\n\u001b[0;32m     99\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_POS_TAGGER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbatch_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\tag\\sequential.pyc\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\tag\\sequential.pyc\u001b[0m in \u001b[0;36mtag_one\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\tag\\sequential.pyc\u001b[0m in \u001b[0;36mchoose_tag\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# higher than that cutoff first; otherwise, return None.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cutoff_prob\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m             \u001b[0mpdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\classify\\maxent.pyc\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\classify\\maxent.pyc\u001b[0m in \u001b[0;36mprob_classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mprob_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logarithmic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\classify\\maxent.pyc\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, featureset, label)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;31m# Known feature name & value:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tokenizing Started at    :     2014-12-29 23:15:14.100000\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "fModel = open(\"C:\\Users\\shubham.akshat\\Desktop\\BODM\\Lemma_synonym_dict.pkl\",\"rb\")\n",
      "lema_to_raw = pickle.load(fModel)\n",
      "fModel.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.cluster import KMeans, MiniBatchKMeans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df[\"mod_fre_txt\"] = \"\"\n",
      "for i, text in enumerate(texts):\n",
      "    temp_line = \"\"\n",
      "    for word in text:\n",
      "        temp_line += str(word) + \" \"\n",
      "    df[\"mod_fre_txt\"][i] = temp_line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer(ngram_range=(1, 2), lowercase = True)\n",
      "tvf  = tv.fit_transform(df[\"mod_fre_txt\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "u'no item named mod_fre_txt'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-86dc64b7748f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtvf\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mod_fre_txt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2001\u001b[0m             \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2002\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2003\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2005\u001b[0m             \u001b[1;31m# duplicate columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    665\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1653\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1655\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1656\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1657\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36m_find_block\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_find_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1935\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_have\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1936\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1937\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36m_check_have\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1940\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_have\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1941\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1942\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no item named %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint_thing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1944\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreindex_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyError\u001b[0m: u'no item named mod_fre_txt'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for n_topics in range(6,21):\n",
      "    \n",
      "    df[\"prediction\"] = \"\"\n",
      "    km = MiniBatchKMeans(n_clusters = int(n_topics)).fit(tvf)\n",
      "    for i in range(len(df)):\n",
      "        s = tv.transform([df[\"mod_fre_txt\"][i]])\n",
      "        df[\"prediction\"][i] = \"cluster \" + str(km.predict(s)[0])\n",
      "        \n",
      "    outfile = \"C:/Users/shubham.akshat/Desktop/BODM//tfidf//cluster_output//topic_modeling_WN \" + str(n_topics) + \"_topics.txt\"\n",
      "    df.to_csv(outfile, sep='\\t', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from __future__ import division\n",
      "dv = DictVectorizer()\n",
      "\n",
      "accu = {}\n",
      "\n",
      "for n_topics in range(6,21):\n",
      "\n",
      "    dfr = pd.read_csv(\"C:\\Users\\shubham.akshat\\Desktop\\BODM/TELEGRAPH__RMA__BODM_26-12.txt\", sep = '\\t')\n",
      "    dfr[\"free_text\"] = dfr.PROBLEM_DESCRIPTION.str.lower()\n",
      "    dfr = dfr[dfr[\"HDX_CODEC\"] == True]\n",
      "\n",
      "    dfr = dfr[pd.notnull(dfr['free_text'])]\n",
      "    dfr = dfr.drop_duplicates(cols = 'free_text')\n",
      "    dfr = dfr.drop([\"PROBLEM_DESCRIPTION\", \"free_text\", \"HDX_CODEC\"], 1)\n",
      "    dfr.index = range(len(dfr))\n",
      "\n",
      "    dfr = dfr.astype(object).fillna(\"BLANK\")\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    dfr[\"prediction\"] = \"\"\n",
      "    \n",
      "    DF = pd.read_csv(\"C:/Users/shubham.akshat/Desktop/BODM//tfidf//cluster_output//topic_modeling_WN \" + str(n_topics) + \"_topics.txt\", sep = '\\t')\n",
      "    lst = DF[\"UID\"].tolist()\n",
      "    \n",
      "    for i in range(len(dfr)):\n",
      "        indx = lst.index(dfr[\"UID\"][i])\n",
      "        dfr[\"prediction\"][i] = DF[\"prediction\"][indx]\n",
      "        \n",
      "        \n",
      "    dfr[\"is_train\"] = np.random.randint(100, size = len(dfr)) <=70\n",
      "    dfr.index = range(len(dfr))\n",
      "    \n",
      "    df1_tr = dfr[dfr[\"is_train\"] == True]\n",
      "    df1_tr.index = range(len(df1_tr))\n",
      "    df1_tr_x = df1_tr\n",
      "    \n",
      "    \n",
      "    df1_tst = dfr[dfr[\"is_train\"] == False]\n",
      "    df1_tst.index = range(len(df1_tst))\n",
      "    df1_tst_x = df1_tst\n",
      "\n",
      "    dfr = dfr.drop([\"is_train\", \"UID\"], 1)\n",
      "    df1_tr = df1_tr.drop([\"is_train\", \"UID\"], 1)\n",
      "    df1_tst = df1_tst.drop([\"is_train\", \"UID\"], 1)\n",
      "    df1_tr_x = df1_tr_x.drop([\"is_train\", \"NDF\", \"UID\"], 1)\n",
      "    df1_tst_x = df1_tst_x.drop([\"is_train\", \"NDF\", \"UID\"], 1)\n",
      "  \n",
      "    \n",
      "    dv.fit(df1_tr.T.to_dict().values())\n",
      "    s = dv.transform(df1_tr_x.T.to_dict().values())\n",
      " #   print s\n",
      "    \n",
      "    LR = LogisticRegression(C = 10).fit(s, df1_tr.NDF)\n",
      "    \n",
      "    \n",
      "    c = dv.transform(df1_tst_x.T.to_dict().values())\n",
      "    \n",
      "    corr = 0\n",
      "    incorr = 0\n",
      "    for i in range(len(df1_tst_x)):\n",
      "#         print LR.predict(c)[i]\n",
      "#         print df1_tst.NDF[i]\n",
      "#         print\n",
      "        if LR.predict(c)[i] == df1_tst.NDF[i]:\n",
      "            corr += 1\n",
      "            flag = 1\n",
      "        else:\n",
      "            incorr +=1\n",
      "            flag = 0\n",
      "\n",
      "    accur = corr/(corr + incorr)\n",
      "    accu[\"cluster \" + str(n_topics)] = accur\n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "{'cluster 10': 0.6486558424274688,\n",
        " 'cluster 11': 0.6491857576593983,\n",
        " 'cluster 12': 0.6490435706695006,\n",
        " 'cluster 13': 0.6444861819157499,\n",
        " 'cluster 14': 0.659437751004016,\n",
        " 'cluster 15': 0.6629549158066268,\n",
        " 'cluster 16': 0.6509921174232128,\n",
        " 'cluster 17': 0.6442151004888648,\n",
        " 'cluster 18': 0.6559719600970612,\n",
        " 'cluster 19': 0.6637649619151251,\n",
        " 'cluster 20': 0.6655108028807682,\n",
        " 'cluster 6': 0.6560988715744224,\n",
        " 'cluster 7': 0.6639365918097754,\n",
        " 'cluster 8': 0.6491847826086956,\n",
        " 'cluster 9': 0.6461118690313779}"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "df1_tr.head()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>COVERAGE_TYPE</th>\n",
        "      <th>GLOBAL_REGION</th>\n",
        "      <th>OWNER_GROUP</th>\n",
        "      <th>PRODUCT_LINE</th>\n",
        "      <th>NDF</th>\n",
        "      <th>prediction</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  RMA DOA</td>\n",
        "      <td> APAC</td>\n",
        "      <td> China TSC</td>\n",
        "      <td> HDX 7002</td>\n",
        "      <td>  True</td>\n",
        "      <td> cluster 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  RMA DOA</td>\n",
        "      <td> APAC</td>\n",
        "      <td>   APAC CC</td>\n",
        "      <td> HDX 9004</td>\n",
        "      <td> False</td>\n",
        "      <td> cluster 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  RMA DOA</td>\n",
        "      <td> APAC</td>\n",
        "      <td> China TSC</td>\n",
        "      <td> HDX 8004</td>\n",
        "      <td>  True</td>\n",
        "      <td> cluster 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> New Inst</td>\n",
        "      <td> APAC</td>\n",
        "      <td>   APAC CC</td>\n",
        "      <td> HDX 8006</td>\n",
        "      <td>  True</td>\n",
        "      <td> cluster 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> New Inst</td>\n",
        "      <td> APAC</td>\n",
        "      <td>   APAC CC</td>\n",
        "      <td> HDX 6000</td>\n",
        "      <td>  True</td>\n",
        "      <td> cluster 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "  COVERAGE_TYPE GLOBAL_REGION OWNER_GROUP PRODUCT_LINE    NDF prediction\n",
        "0       RMA DOA          APAC   China TSC     HDX 7002   True  cluster 0\n",
        "1       RMA DOA          APAC     APAC CC     HDX 9004  False  cluster 0\n",
        "2       RMA DOA          APAC   China TSC     HDX 8004   True  cluster 0\n",
        "3      New Inst          APAC     APAC CC     HDX 8006   True  cluster 0\n",
        "4      New Inst          APAC     APAC CC     HDX 6000   True  cluster 0"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Total number of input features:: \" + \"\\t\",len(LR.coef_[0])\n",
      "coeffs = pd.Series(LR.coef_[0].copy(), index = dv.feature_names_)\n",
      "coeffs.sort()\n",
      "print coeffs\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of input features:: \t95\n",
        "GLOBAL_REGION=BLANK                    -3.874648\n",
        "OWNER_GROUP=EMEA Service Delivery      -2.205562\n",
        "OWNER_GROUP=NALA Iformata              -2.156295\n",
        "OWNER_GROUP=NALA Endpoint Escalation   -2.083070\n",
        "OWNER_GROUP=Default Organization       -1.833416\n",
        "OWNER_GROUP=NALA - FSE RTS             -1.394647\n",
        "COVERAGE_TYPE=CS-OOW                   -1.314252\n",
        "COVERAGE_TYPE=REM                      -1.305641\n",
        "OWNER_GROUP=NALA Service Delivery      -1.256808\n",
        "COVERAGE_TYPE=SSN                      -1.202290\n",
        "COVERAGE_TYPE=4HRDepot                 -1.145466\n",
        "OWNER_GROUP=NALA FSE East              -0.967597\n",
        "PRODUCT_LINE=HDX 9002                  -0.959640\n",
        "PRODUCT_LINE=HDX 9001                  -0.934874\n",
        "PRODUCT_LINE=HDX 9004                  -0.881778\n",
        "COVERAGE_TYPE=OOW3                     -0.795923\n",
        "prediction=topic 11                    -0.780039\n",
        "COVERAGE_TYPE=ENTWAR                   -0.595053\n",
        "OWNER_GROUP=Demo Support Group         -0.538651\n",
        "prediction=topic 6                     -0.480138\n",
        "prediction=topic 15                    -0.451470\n",
        "OWNER_GROUP=Network TSC                -0.418959\n",
        "COVERAGE_TYPE=NEWR                     -0.409562\n",
        "prediction=topic 5                     -0.376415\n",
        "PRODUCT_LINE=HDX 4001                  -0.322994\n",
        "PRODUCT_LINE=HDX 8006                  -0.309752\n",
        "OWNER_GROUP=APAC Telepresence          -0.273357\n",
        "COVERAGE_TYPE=OWEE                     -0.249962\n",
        "prediction=topic 18                    -0.244544\n",
        "COVERAGE_TYPE=OW30                     -0.243135\n",
        "prediction=topic 16                    -0.241873\n",
        "OWNER_GROUP=PLCM CMC                   -0.234942\n",
        "OWNER_GROUP=NALA CC                    -0.232887\n",
        "PRODUCT_LINE=HDX 6000                  -0.194249\n",
        "COVERAGE_TYPE=Standard                 -0.192124\n",
        "PRODUCT_LINE=HDX 4002                  -0.180998\n",
        "OWNER_GROUP=China TSC                  -0.158803\n",
        "prediction=topic 2                     -0.134198\n",
        "prediction=topic 4                     -0.127022\n",
        "PRODUCT_LINE=HDX 7001                  -0.126321\n",
        "PRODUCT_LINE=HDX 7002                  -0.109186\n",
        "prediction=topic 17                    -0.107995\n",
        "COVERAGE_TYPE=NBDAM                    -0.102562\n",
        "prediction=topic 19                    -0.070319\n",
        "OWNER_GROUP=NALA Escalation            -0.068231\n",
        "PRODUCT_LINE=HDX 9006                  -0.057506\n",
        "PRODUCT_LINE=HDX 8004                  -0.056352\n",
        "GLOBAL_REGION=SOUTH AMERICA            -0.053931\n",
        "prediction=topic 9                     -0.039105\n",
        "prediction=topic 8                     -0.024727\n",
        "prediction=topic 0                     -0.014237\n",
        "NDF                                     0.000000\n",
        "prediction=topic 3                      0.045530\n",
        "prediction=topic 14                     0.066830\n",
        "OWNER_GROUP=EMEA CC                     0.067284\n",
        "OWNER_GROUP=NALA Glowpoint              0.078425\n",
        "OWNER_GROUP=NALA Elite                  0.079647\n",
        "PRODUCT_LINE=HDX 7006                   0.099603\n",
        "OWNER_GROUP=EMEA TSC                    0.108386\n",
        "prediction=topic 1                      0.109783\n",
        "OWNER_GROUP=Endpoint TSC                0.111851\n",
        "prediction=topic 12                     0.116364\n",
        "OWNER_GROUP=EMEA Elite                  0.143816\n",
        "OWNER_GROUP=EMEA PGS Ops                0.159995\n",
        "OWNER_GROUP=India TSC                   0.176305\n",
        "OWNER_GROUP=APAC CC                     0.202564\n",
        "OWNER_GROUP=NALA FSE West               0.209094\n",
        "COVERAGE_TYPE=New Inst                  0.260512\n",
        "COVERAGE_TYPE=Warranty                  0.267724\n",
        "OWNER_GROUP=NALA PGS Ops                0.274016\n",
        "COVERAGE_TYPE=OOW10                     0.276962\n",
        "PRODUCT_LINE=HDX 4500                   0.280644\n",
        "prediction=topic 7                      0.298257\n",
        "COVERAGE_TYPE=IWEE                      0.341397\n",
        "prediction=topic 10                     0.365651\n",
        "prediction=topic 13                     0.438282\n",
        "OWNER_GROUP=APAC Escalation             0.509232\n",
        "GLOBAL_REGION=APAC                      0.535173\n",
        "COVERAGE_TYPE=ENTAGR                    0.559047\n",
        "COVERAGE_TYPE=Prem                      0.592015\n",
        "COVERAGE_TYPE=PremPlus                  0.600036\n",
        "OWNER_GROUP=ROA TSC                     0.617104\n",
        "OWNER_GROUP=EMEA Escalation             0.686219\n",
        "GLOBAL_REGION=EMEA                      0.840523\n",
        "GLOBAL_REGION=NORTH AMERICA             0.901497\n",
        "COVERAGE_TYPE=HWORR10                   0.927611\n",
        "COVERAGE_TYPE=CS- E Exch                0.937816\n",
        "PRODUCT_LINE=HDX 4003                   0.938907\n",
        "OWNER_GROUP=Japan TSC                   0.982855\n",
        "COVERAGE_TYPE=RMA DOA                   1.141465\n",
        "PRODUCT_LINE=HDX 4000 HD                1.163110\n",
        "OWNER_GROUP=HP OMC                      1.552417\n",
        "OWNER_GROUP=EMEA 3rd Party              1.643277\n",
        "OWNER_GROUP=NALA Engagement             2.270592\n",
        "OWNER_GROUP=CMS T3 SysOps               2.298757\n",
        "Length: 95, dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from __future__ import division\n",
      "dv = DictVectorizer()\n",
      "\n",
      "accu = {}\n",
      "\n",
      "for n_topics in range(6,21):\n",
      "\n",
      "    dfr = pd.read_csv(\"C:\\Users\\shubham.akshat\\Desktop\\BODM/TELEGRAPH__RMA__BODM_26-12.txt\", sep = '\\t')\n",
      "    dfr[\"free_text\"] = dfr.PROBLEM_DESCRIPTION.str.lower()\n",
      "    dfr = dfr[dfr[\"HDX_CODEC\"] == True]\n",
      "\n",
      "    dfr = dfr[pd.notnull(dfr['free_text'])]\n",
      "    dfr = dfr.drop_duplicates(cols = 'free_text')\n",
      "    dfr = dfr.drop([\"PROBLEM_DESCRIPTION\", \"free_text\", \"HDX_CODEC\"], 1)\n",
      "    dfr.index = range(len(dfr))\n",
      "\n",
      "    dfr = dfr.astype(object).fillna(\"BLANK\")\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    dfr[\"prediction\"] = \"\"\n",
      "    \n",
      "    DF = pd.read_csv(\"C:/Users/shubham.akshat/Desktop/BODM//tfidf//topic_modeling_WN_TOT \" + str(n_topics) + \"_topics.txt\", sep = '\\t')\n",
      "    lst = DF[\"UID\"].tolist()\n",
      "    \n",
      "    for i in range(len(dfr)):\n",
      "        indx = lst.index(dfr[\"UID\"][i])\n",
      "        dfr[\"prediction\"][i] = DF[\"Top topic\"][indx]\n",
      "        \n",
      "        \n",
      "    dfr[\"is_train\"] = np.random.randint(100, size = len(dfr)) <=70\n",
      "    dfr.index = range(len(dfr))\n",
      "    \n",
      "    df1_tr = dfr[dfr[\"is_train\"] == True]\n",
      "    df1_tr.index = range(len(df1_tr))\n",
      "    df1_tr_x = df1_tr\n",
      "    \n",
      "    \n",
      "    df1_tst = dfr[dfr[\"is_train\"] == False]\n",
      "    df1_tst.index = range(len(df1_tst))\n",
      "    df1_tst_x = df1_tst\n",
      "\n",
      "    dfr = dfr.drop([\"is_train\", \"UID\"], 1)\n",
      "    df1_tr = df1_tr.drop([\"is_train\", \"UID\"], 1)\n",
      "    df1_tst = df1_tst.drop([\"is_train\", \"UID\"], 1)\n",
      "    df1_tr_x = df1_tr_x.drop([\"is_train\", \"NDF\", \"UID\"], 1)\n",
      "    df1_tst_x = df1_tst_x.drop([\"is_train\", \"NDF\", \"UID\"], 1)\n",
      "  \n",
      "    \n",
      "    dv.fit(df1_tr.T.to_dict().values())\n",
      "    s = dv.transform(df1_tr_x.T.to_dict().values())\n",
      " #   print s\n",
      "    \n",
      "    LR = LogisticRegression(C = 10).fit(s, df1_tr.NDF)\n",
      "    \n",
      "    \n",
      "    c = dv.transform(df1_tst_x.T.to_dict().values())\n",
      "    \n",
      "    corr = 0\n",
      "    incorr = 0\n",
      "    for i in range(len(df1_tst_x)):\n",
      "#         print LR.predict(c)[i]\n",
      "#         print df1_tst.NDF[i]\n",
      "#         print\n",
      "        if LR.predict(c)[i] == df1_tst.NDF[i]:\n",
      "            corr += 1\n",
      "            flag = 1\n",
      "        else:\n",
      "            incorr +=1\n",
      "            flag = 0\n",
      "\n",
      "    accur = corr/(corr + incorr)\n",
      "    accu[\"Topic \" + str(n_topics)] = accur\n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "File C:/Users/shubham.akshat/Desktop/BODM//tfidf//topic_modeling_WN_TOT 15_topics.txt does not exist",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-12-056536145467>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mdfr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"prediction\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/shubham.akshat/Desktop/BODM//tfidf//topic_modeling_WN_TOT \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_topics.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mlst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"UID\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    955\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\parser.pyd\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:2987)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\parser.pyd\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:5345)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: File C:/Users/shubham.akshat/Desktop/BODM//tfidf//topic_modeling_WN_TOT 15_topics.txt does not exist"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "{'Topic 10': 0.6469333333333334,\n",
        " 'Topic 11': 0.660048952950775,\n",
        " 'Topic 12': 0.6523394994559304,\n",
        " 'Topic 13': 0.655733261046354,\n",
        " 'Topic 14': 0.6610169491525424,\n",
        " 'Topic 6': 0.6544432548179872,\n",
        " 'Topic 7': 0.6639103761002935,\n",
        " 'Topic 8': 0.655005382131324,\n",
        " 'Topic 9': 0.6471397139713971}"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df1_tr.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>COVERAGE_TYPE</th>\n",
        "      <th>GLOBAL_REGION</th>\n",
        "      <th>OWNER_GROUP</th>\n",
        "      <th>PRODUCT_LINE</th>\n",
        "      <th>NDF</th>\n",
        "      <th>REPEAT</th>\n",
        "      <th>PARTNER</th>\n",
        "      <th>prediction</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  RMA DOA</td>\n",
        "      <td> APAC</td>\n",
        "      <td> China TSC</td>\n",
        "      <td> HDX 7001</td>\n",
        "      <td> False</td>\n",
        "      <td>  True</td>\n",
        "      <td> False</td>\n",
        "      <td> topic 10</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  RMA DOA</td>\n",
        "      <td> APAC</td>\n",
        "      <td> China TSC</td>\n",
        "      <td> HDX 7002</td>\n",
        "      <td>  True</td>\n",
        "      <td>  True</td>\n",
        "      <td> False</td>\n",
        "      <td> topic 10</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  RMA DOA</td>\n",
        "      <td> APAC</td>\n",
        "      <td> China TSC</td>\n",
        "      <td> HDX 6000</td>\n",
        "      <td> False</td>\n",
        "      <td>  True</td>\n",
        "      <td> False</td>\n",
        "      <td>  topic 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  RMA DOA</td>\n",
        "      <td> APAC</td>\n",
        "      <td> China TSC</td>\n",
        "      <td> HDX 8004</td>\n",
        "      <td>  True</td>\n",
        "      <td>  True</td>\n",
        "      <td> False</td>\n",
        "      <td> topic 11</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> New Inst</td>\n",
        "      <td> APAC</td>\n",
        "      <td>   APAC CC</td>\n",
        "      <td> HDX 8006</td>\n",
        "      <td>  True</td>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "      <td>  topic 8</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "  COVERAGE_TYPE GLOBAL_REGION OWNER_GROUP PRODUCT_LINE    NDF REPEAT PARTNER prediction\n",
        "0       RMA DOA          APAC   China TSC     HDX 7001  False   True   False   topic 10\n",
        "1       RMA DOA          APAC   China TSC     HDX 7002   True   True   False   topic 10\n",
        "2       RMA DOA          APAC   China TSC     HDX 6000  False   True   False    topic 0\n",
        "3       RMA DOA          APAC   China TSC     HDX 8004   True   True   False   topic 11\n",
        "4      New Inst          APAC     APAC CC     HDX 8006   True  False   False    topic 8"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df1_tr.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "(9061, 8)"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}