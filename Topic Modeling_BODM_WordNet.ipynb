{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import gensim"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import nltk\n",
      "import numpy as np\n",
      "import gensim\n",
      "import random\n",
      "from datetime import datetime\n",
      "from gensim import corpora, models, similarities\n",
      "from nltk.tokenize.punkt import PunktWordTokenizer\n",
      "\n",
      "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "import unicodedata\n",
      "\n",
      "def unaccent( text ):\n",
      "    s = unicodedata.normalize( 'NFKD', text )\n",
      "    s = ''.join( c for c in s if ord( c ) < 127 )\n",
      "    return s\n",
      "\n",
      "stemmer = PorterStemmer()\n",
      "lmtzr = WordNetLemmatizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_rw = pd.read_csv(\"C:\\Users\\shubham.akshat\\Desktop\\BODM/remove_words.txt\", sep = \"\\t\")\n",
      "remove_words = df_rw[\"remove_words\"].tolist()\n",
      "remove_words_lst = []\n",
      "for item in remove_words:\n",
      "    remove_words_lst.append(item.lower())\n",
      "\n",
      "df_wm = pd.read_csv(\"C:\\Users\\shubham.akshat\\Desktop\\BODM/word_mapping.txt\", sep = \"\\t\")\n",
      "incorrect_words = df_wm[\"incorrect\"].tolist()\n",
      "correct_words = df_wm[\"correct\"].tolist()\n",
      "\n",
      "def get_correct_word(word):\n",
      "    index = incorrect_words.index(word)\n",
      "    return correct_words[index]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn\n",
      "import itertools\n",
      "nltk.tag._POS_TAGGER\n",
      "\n",
      "def get_wordnet_pos(treebank_tag):\n",
      "    if treebank_tag.startswith('J'):\n",
      "        return wn.ADJ\n",
      "    elif treebank_tag.startswith('V'):\n",
      "        return wn.VERB\n",
      "    elif treebank_tag.startswith('N'):\n",
      "        return wn.NOUN\n",
      "    elif treebank_tag.startswith('R'):\n",
      "        return wn.ADV\n",
      "    else:\n",
      "        return wn.NOUN\n",
      "\n",
      "\n",
      "def get_first_synonym(lemma0, pos):\n",
      "    synonyms = []\n",
      "    if len(wn.synsets(lemma0, pos)) > 0:\n",
      "        for synset in wn.synsets(lemma0, pos):\n",
      "            synonyms.append(synset.lemma_names)\n",
      "            merged = list(itertools.chain(*synonyms))\n",
      "            merged.sort()\n",
      "        return merged[0].lower()\n",
      "    else:\n",
      "        return lemma0.lower()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stem_tokens(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        stemmed.append(stemmer.stem(item))\n",
      "    return stemmed\n",
      "\n",
      "def lemma_tokens(tokens, lmtzr):\n",
      "    lemma_lst = []\n",
      "    for word in tokens:\n",
      "        lexicon = nltk.pos_tag([word])\n",
      "        POS = get_wordnet_pos(lexicon[0][1])        \n",
      "        lemma_lst.append(lmtzr.lemmatize(word, POS))\n",
      "    return lemma_lst\n",
      "\n",
      "\n",
      "def tokenize(text):\n",
      "    \n",
      "    tokens = nltk.word_tokenize(text)\n",
      "   # filtered_tokens = [w for w in tokens if not w in stopwords.words('english')]\n",
      "\n",
      "    \n",
      "    lexicons = nltk.pos_tag(tokens)\n",
      "    lemmatised_tokens = []\n",
      "    for lexicon in lexicons:\n",
      "        if lexicon[0].lower() in incorrect_words:\n",
      "            lexicon = (get_correct_word(lexicon[0]), lexicon[1])\n",
      "        if lexicon[0].lower() not in remove_words and lexicon[0] not in stopwords.words('english'):\n",
      "            wn_pos = get_wordnet_pos(lexicon[1])\n",
      "            lemma0 = lmtzr.lemmatize(lexicon[0].lower(), wn_pos)\n",
      "            lemma1 = get_first_synonym(lemma0, wn_pos)\n",
      "            lemmatised_tokens.append(lemma1)\n",
      "    \n",
      " #   lemmatised_tokens = [w for w in lemmatised_tokens_1 if not w in remove_words]\n",
      "    \n",
      "#    return stemmed_tokens\n",
      "    return lemmatised_tokens\n",
      "#    return filtered_tokens\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(\"C:\\Users\\shubham.akshat\\Desktop\\BODM/TELEGRAPH__RMA__BODM.txt\", sep = \"\\t\")\n",
      "\n",
      "df[\"free_text\"] = df.PROBLEM_DESCRIPTION.str.lower()\n",
      "df = df[df[\"HDX_CODEC\"] == True]\n",
      "\n",
      "df = df[pd.notnull(df['free_text'])]\n",
      "df = df.drop_duplicates(cols = 'free_text')\n",
      "df.index = range(len(df))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "12719"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "str(\"Ec04\") in [\"ec04\", \"ec05\", \"ec06\", \"ec07\", \"ec08\", \"ec09\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "replace_punctuation = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
      "print \"Tokenizing Started at    :    \", datetime.now()\n",
      "\n",
      "texts = []\n",
      "for i in range(len(df)):\n",
      "    temp1 = str(df[\"free_text\"][i]).translate(replace_punctuation)\n",
      "    temp2 = temp1.decode('utf-8','ignore')\n",
      "    temp3 = tokenize(temp2)\n",
      "    temp4 = []\n",
      "    flag = {}\n",
      "    for word in temp3:\n",
      "        if str(word) in [\"ec04\", \"ec05\", \"ec06\", \"ec07\", \"ec08\", \"ec09\"]:\n",
      "            flag[word] = 1\n",
      "        elif any(char.isdigit() for char in word) or len(word) < 3:\n",
      "            flag[word] = 0\n",
      "        else:\n",
      "            flag[word] = 1\n",
      "    for word, flg in flag.items():\n",
      "        if flg == 1:\n",
      "            temp4.append(word)\n",
      "        \n",
      "    texts.append(temp4)\n",
      "\n",
      "print \"Tokenizing ended at    :    \", datetime.now()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tokenizing Started at    :     2014-12-27 00:12:19.550000\n",
        "Tokenizing ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:16:20.138000\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "\n",
      "lema_to_raw = defaultdict(str)\n",
      "\n",
      "def tokenize_(text):\n",
      "    \n",
      "    tokens = nltk.word_tokenize(text)\n",
      " #   filtered_tokens = [w for w in tokens if not w in stopwords.words('english')]\n",
      "\n",
      "    \n",
      "    lexicons = nltk.pos_tag(tokens)\n",
      "#    lemmatised_tokens_1 = []\n",
      "    \n",
      "    for lexicon in lexicons:\n",
      "        wn_pos = get_wordnet_pos(lexicon[1])\n",
      "        lemma0 = lmtzr.lemmatize(lexicon[0], wn_pos)\n",
      "        if lemma0 in incorrect_words:\n",
      "            lemma0 = get_correct_word(lemma0)\n",
      "        if lemma0 not in remove_words and lemma0 not in stopwords.words('english'):            \n",
      "            lemma1 = get_first_synonym(lemma0, wn_pos)\n",
      "            lema_to_raw[lemma1] += str(lemma0) + str(\" , \")\n",
      "         #   lemmatised_tokens_1.append(lema_to_raw)\n",
      "     \n",
      "    return lema_to_raw\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for i in range(len(df)):\n",
      "    temp1 = str(df[\"free_text\"][i]).translate(replace_punctuation)\n",
      "    temp2 = temp1.decode('utf-8','ignore')\n",
      "    temp3 = tokenize_(temp2)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "fModel = open(\"C:\\Users\\shubham.akshat\\Desktop\\BODM\\Lemma_synonym_dict.pkl\",\"wb\")\n",
      "pickle.dump(lema_to_raw, fModel,1)\n",
      "fModel.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"dict. and word counting started at    :    \", datetime.now()\n",
      "dictionary = corpora.Dictionary(texts)\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "print \"dict. and word counting ended at    :    \", datetime.now()\n",
      "\n",
      "print \"Dictionary length:   \",len(dictionary)\n",
      "\n",
      "\n",
      "tfidf = gensim.models.TfidfModel(corpus, normalize = True)\n",
      "corpus_tfidf = tfidf[corpus]\n",
      "\n",
      "\n",
      "# n_topics = raw_input(\"enter # of topics:::::\")\n",
      "# n_topics = int(n_topics)\n",
      "\n",
      "# print \"Modeling Started at    :    \", datetime.now()\n",
      "# lda = gensim.models.LdaModel(corpus_tfidf, id2word = dictionary, num_topics = n_topics)\n",
      "# print \"Modeling ended at    :    \", datetime.now()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dict. and word counting started at    :     2014-12-27 00:22:29.561000\n",
        "dict. and word counting ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:22:30.391000\n",
        "Dictionary length:    4886\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#  FOR TFIDF\n",
      "\n",
      "print \"update DONE\"\n",
      "import pickle\n",
      "lst = [6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
      "for n_topics in lst:\n",
      "    n_topics = int(n_topics)\n",
      "\n",
      "    print \"Modeling Started at    :    \", datetime.now()\n",
      "    lda = gensim.models.LdaModel(corpus_tfidf, id2word = dictionary, num_topics = n_topics)\n",
      "    print \"Modeling ended at    :    \", datetime.now()\n",
      "\n",
      "    classifier_dict = {}\n",
      "    classifier_dict.setdefault(str(n_topics)+\"_topics\", [lda, dictionary, tfidf])\n",
      "\n",
      "    fModel = open(\"C:\\Users\\shubham.akshat\\Desktop\\BODM\\\\tfidf\\\\LDA_classifiers_WN \" + str(n_topics) + \" topics.pkl\",\"wb\")\n",
      "    pickle.dump(classifier_dict, fModel,1)\n",
      "    fModel.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "update DONE\n",
        "Modeling Started at    :     2014-12-27 00:23:14.278000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:23:23.116000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:23:23.241000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:23:32.206000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:23:32.343000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:23:39.996000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:23:40.121000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:23:47.707000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:23:47.857000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:23:54.499000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:23:54.684000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:02.149000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:02.269000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:08.994000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:09.114000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:15.662000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:15.809000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:23.437000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:23.552000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:29.980000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:30.100000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:36.991000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:37.116000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:44.215000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:44.340000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:50.960000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:51.081000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:57.531000\n",
        "Modeling Started at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:24:57.651000\n",
        "Modeling ended at    :    "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014-12-27 00:25:03.881000\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lema_to_raw[\"bang\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "'boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , screw , boot , boot , boot , hit , boot , hit , hit , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , bang , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , hit , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , kick , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , boot , '"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(20):\n",
      "    temp = lda.show_topic(topicid = i, topn = 20)\n",
      "    terms = []\n",
      "    for term in temp:\n",
      "     #   s = lema_to_raw[term[1]].split(\",\")[0]\n",
      "        s = term[1]\n",
      "        terms.append(s)\n",
      " #   print terms\n",
      "    print \"Top 10 terms for topic #\" + str(i) + \": \"+ \", \".join(terms)\n",
      "    print \n",
      "\n",
      "print \"DDDDoneeeee\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 10 terms for topic #0: bread_and_butter, asking, all_in, begin, loud, boot, computer_hardware, home, demo, abide_by, fail, ability, buff, cross-file, dissonance, cma, projector, tech, encounter, arrive_at\n",
        "\n",
        "Top 10 terms for topic #1: boot, longer, ask, action, drop_off, ability, antiphonal, disconnect, client, barge_in, fan, frequently, associate, attach, apparatus, device, act, reset, bid, rca\n",
        "\n",
        "Top 10 terms for topic #2: monitor, reboots, display, background, dvi, end_product, vnoc, double, bill_of_fare, hdmi, sign, embrasure, acclivity, continue, adjudicate, cable, continously, restart, socket, crt_screen\n",
        "\n",
        "Top 10 terms for topic #3: block, doa, nic, close, speaker, double, establish, ego, call, crt_screen, asking, black, trouble-shoot, premier, audio, rev, soundbar, bar, absolved, degree\n",
        "\n",
        "Top 10 terms for topic #4: adhere, electronic_network, device, embrasure, lan, dab, crt_screen, abide, doesnt, concluding, associate, act, connectivity, anything, association, logo, demand, knock, answer, accomplish\n",
        "\n",
        "Top 10 terms for topic #5: bang, buff, dab, crt_screen, add_up, discover, dissonance, logo, boot, chief, arrive_at, authorise, freeze, rebooting, cdec, become, amply, affair, arrest, curl\n",
        "\n",
        "Top 10 terms for topic #6: dsp, betray, computer_memory, computer_error, exam, audio, ec09, content, closed_circuit, consequence, boot, bankrupt, dead, stuck, arrest, bang, acquire, bankruptcy, reboot, condition\n",
        "\n",
        "Top 10 terms for topic #7: bad, alternate, call, bundle, bespeak, caliber, departure, embrasure, monitor, bear_witness, old, all, acquire, encounter, ethernet, bear_on, drug_user, become, outage, acknowledge\n",
        "\n",
        "Top 10 terms for topic #8: exchange, advance, update, comment, computer_software, basal, out_of_reach, site, assay, robert_william_service, tpx, config, act, etc, bring_through, deloitte, camera, capacity, lcd, create\n",
        "\n",
        "Top 10 terms for topic #9: blink, install, aristocratic, swap, arcsecond, christ_within, dial, electric_switch, connectedness, chair, just_the_ticket, ability, break, amytal, call, data, abstemious, boot, act, boring\n",
        "\n",
        "Top 10 terms for topic #10: www, access, beginning, clip, interface, ceaselessly, call, cast, bankruptcy, address, arbitrarily, bechance, center, inauguration, cause, adjust, cease, end_point, noisy, boot\n",
        "\n",
        "Top 10 terms for topic #11: ability, microphone, audio, act, christ_within, english, mic, chair, federal_reserve_note, cord, appearance, abstemious, embrasure, cso, aberration, serial, answer, sign, flash, battlefront\n",
        "\n",
        "Top 10 terms for topic #12: ec07, configuration, button, free, accompaniment, closet, twice, aright, network, equipment, contact, configure, expert, adjure, subwoofer, radio, association, alternative, associate, ability\n",
        "\n",
        "Top 10 terms for topic #13: adv, arcminute, electric_outlet, holy_order, get, call, codecs, tested, dmos, conference, fails, bead, chipset, chatter, reboots, shade, boot, inside, random, forcible\n",
        "\n",
        "Top 10 terms for topic #14: become, readjust, account, detect, celebrate, local, affirm, accept, former, republic_of_singapore, initial, camera, multipoint, replace, care, dont, flick, call, bite, either\n",
        "\n",
        "Top 10 terms for topic #15: premier, stuck, dab, logo, crt_screen, boot, appendage, express, job, fashion, bang, ec08, acquire, reboot, reboots, pci, software, keystone_state, become, call\n",
        "\n",
        "Top 10 terms for topic #16: afford, booting, adulterate, body_of_work, continually, boot, brassy, crestron, yes, advertise, adaptation, case, ability, rattle, chink, burden, startup, reset, affect, late\n",
        "\n",
        "Top 10 terms for topic #17: power, video, capacity, camera, re-start, building_block, answer, unresponsive, apportion, output, green, send, dark-green, act, acquire, locally, half, double, monitor, around\n",
        "\n",
        "Top 10 terms for topic #18: add-in, bear_witness, red, end_product, double, camera, monitor, act, restarts, agate_line, air, malcolm_stock, embrasure, aristocratic, dark-green, display, crt_screen, bad, electronic_network, color\n",
        "\n",
        "Top 10 terms for topic #19: acquire, appear, damage, ascertain, block_out, box, accredit, ability, arouse, back, department_of_state, camera, embrasure, plug, achiever, default, garden_pink, act, black, junior-grade\n",
        "\n",
        "DDDDoneeeee\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(20):\n",
      "    temp = lda.show_topic(topicid = i, topn = 20)\n",
      "    terms = []\n",
      "    for term in temp:\n",
      "        s = lema_to_raw[term[1]].split(\",\")[0]\n",
      "     #   s = term[1]\n",
      "        terms.append(s)\n",
      " #   print terms\n",
      "    print \"Top 10 terms for topic #\" + str(i) + \": \"+ \", \".join(terms)\n",
      "    print \n",
      "\n",
      "print \"DDDDoneeeee\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 10 terms for topic #0: , request , dead , start , loud , boot , hardware , internal , demo , follow , fail , power , fan , register , noise , cma , projector , tech , receive , hit \n",
        "\n",
        "Top 10 terms for topic #1: boot , longer , require , activity , lose , power , responsive , disconnect , client , crash , fan , frequently , connect , attach , setup , device , turn , reset , play , rca \n",
        "\n",
        "Top 10 terms for topic #2: monitor , reboots , display , setting , dvi , output , vnoc , image , menu , hdmi , signal , port , upgrade , remain , resolve , cable , continously , restart , socket , screen \n",
        "\n",
        "Top 10 terms for topic #3: freeze , doa , nic , close , speaker , image , establish , self , call , screen , request , black , troubleshoot , premier , sound , rev , soundbar , bar , clear , stage \n",
        "\n",
        "Top 10 terms for topic #4: stick , network , device , port , lan , splash , screen , support , doesnt , terminal , connect , turn , connectivity , anything , connection , logo , , ping , response , achieve \n",
        "\n",
        "Top 10 terms for topic #5: boot , fan , splash , screen , come , hear , noise , logo , boot , main , hit , pass , freeze , rebooting , cdec , turn , fully , function , stop , lock \n",
        "\n",
        "Top 10 terms for topic #6: dsp , fail , memory , error , test , sound , ec09 , content , loop , import , boot , break , dead , stuck , stop , boot , win , failure , reboot , status \n",
        "\n",
        "Top 10 terms for topic #7: bad , alternate , call , packet , signal , quality , loss , port , monitor , show , old , completely , win , receive , ethernet , continue , user , turn , outage , notice \n",
        "\n",
        "Top 10 terms for topic #8: replace , upgrade , update , input , software , primary , unreachable , site , seek , service , tpx , config , turn , etc , save , deloitte , camera , content , lcd , create \n",
        "\n",
        "Top 10 terms for topic #9: blinking , install , blue , swap , , light , dial , switch , link , lead , ticket , power , fault , blue , call , information , light , boot , turn , slow \n",
        "\n",
        "Top 10 terms for topic #10: web , access , start , time , interface , continuously , call , cast , failure , address , randomly , , eye , startup , cause , set , end , endpoint , noisy , boot \n",
        "\n",
        "Top 10 terms for topic #11: power , microphone , sound , turn , light , side , mic , lead , bill , cord , show , light , port , cso , distortion , serial , response , signal , flashing , front \n",
        "\n",
        "Top 10 terms for topic #12: ec07 , configuration , button , loose , support , press , twice , right , network , equipment , touch , configure , technical , press , subwoofer , wireless , connection , option , connect , power \n",
        "\n",
        "Top 10 terms for topic #13: adv , minute , outlet , order , , call , codecs , tried , dmos , conference , fails , , chipset , click , reboots , tint , boot , inside , random , physical \n",
        "\n",
        "Top 10 terms for topic #14: turn , reset , describe , detect , , local , verify , take , previous , singapore , initial , camera , multipoint , replace , like , dont , flicker , call , burn , either \n",
        "\n",
        "Top 10 terms for topic #15: premier , stuck , splash , logo , screen , boot , process , state , , mode , boot , ec08 , win , reboot , reboots , pci , software , pas , turn , call \n",
        "\n",
        "Top 10 terms for topic #16: open , booting , load , work , continually , boot , loud , crestron , yes , push , version , type , power , rattle , click , load , startup , reset , impact , recent \n",
        "\n",
        "Top 10 terms for topic #17: power , video , content , camera , restart , , response , unresponsive , share , output , green , send , green , turn , win , locally , half , image , monitor , around \n",
        "\n",
        "Top 10 terms for topic #18: board , show , red , output , image , camera , monitor , turn , restarts , line , transmit , stock , port , blue , green , display , screen , bad , network , color \n",
        "\n",
        "Top 10 terms for topic #19: win , appear , damage , find , screen , box , recognize , power , wake , back , state , camera , port , plug , success , default , pink , turn , black , secondary \n",
        "\n",
        "DDDDoneeeee\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BELOW CODE IS FOR FINDING THE CONTEXT IN WHICH A CERTAIN WORD HAS APPEARED!!!! "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(\"C:\\Users\\shubham.akshat\\Desktop\\BODM/TELEGRAPH__RMA__BODM.txt\", sep = \"\\t\")\n",
      "\n",
      "df[\"PROBLEM_DESCRIPTION\"] = df.PROBLEM_DESCRIPTION.str.lower()\n",
      "df = df[df[\"HDX_CODEC\"] == True]\n",
      "\n",
      "df = df[pd.notnull(df['PROBLEM_DESCRIPTION'])]\n",
      "df = df.drop_duplicates(cols = 'PROBLEM_DESCRIPTION')\n",
      "df.index = range(len(df))\n",
      "\n",
      "import nltk\n",
      "text = []\n",
      "for i in range(len(df)):\n",
      "    \n",
      "    s = nltk.word_tokenize(df[\"PROBLEM_DESCRIPTION\"][i])\n",
      "    text += s\n",
      "    \n",
      "    \n",
      "text1 = nltk.Text(text)\n",
      "fdist = nltk.FreqDist(text1)\n",
      "print type(fdist)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word = \"ec07\"\n",
      "print \":::::::\",fdist[word]\n",
      "text1.concordance(word, 100, 150)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}